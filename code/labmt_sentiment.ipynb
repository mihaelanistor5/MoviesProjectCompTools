{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize, RegexpTokenizer \n",
    "from labMTsimple.storyLab import emotionFileReader, emotion, stopper, emotionV\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "labMT,labMTvector,labMTwordList = emotionFileReader(stopval=0.0,lang='english',returnVector=True)\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the reviews for each movie are stored in a separate csv file \n",
    "#for efficiency reasons, we collect files together in a dictionary where the movie name is the key and the content of the file(reviews) is the value\n",
    "movie_files_path = os.getcwd()+ '\\\\dataset_review' + '\\\\2_reviews_per_movie_raw\\\\'\n",
    "reviews_dict = {}\n",
    "for file in os.listdir(movie_files_path):\n",
    "    movie_name = file.replace('.csv', '')\n",
    "    movie_name = movie_name.replace('_',':')\n",
    "    movie_name = movie_name[:-5]\n",
    "    #read csv document for each movie \n",
    "    file_content = pd.read_csv(movie_files_path + file, encoding = \"ISO-8859-1\").convert_dtypes().to_dict()\n",
    "    reviews_dict[movie_name] = file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally the dictionary is processed into a dataframe \n",
    "movie_reviews_raw = pd.DataFrame.from_dict(reviews_dict,orient='index').convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we rank the reviewers based on the number of reviews left and keep the ones with 5 or more movies reviewed \n",
    "popular_reviewers = pd.read_pickle(r'new_num_review_user.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_reviewers_list = popular_reviewers[popular_reviewers['revs_num']>=5]['username'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews_raw['original_title'] = movie_reviews_raw.index\n",
    "movie_reviews_raw = movie_reviews_raw[['username','rating','helpful','total','date','title','review','original_title']].reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = movie_reviews_raw[['original_title','username','rating','title', 'review']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmdb_5000 dataset contains more attributes which can be used as features for the recommender systems \n",
    "tmdb_5000 =pd.read_csv(\"C:\\\\Users\\\\mihae\\\\projects\\\\MoviesProjectCompTools\\\\dataset_5000\\\\tmdb_5000_movies.csv\")['original_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we keep only the movies that appear in both tmdb_5000_movies and the movie review dataset \n",
    "movie_reviews = movie_reviews[movie_reviews['original_title'].isin(list(tmdb_5000))].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tmdb_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reviews_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove <br/> from text \n",
    "def process_text(dict_review):\n",
    "    \n",
    "    new_dict = {}\n",
    "    for k,text in dict_review.items():\n",
    "        new_dict[k] = re.sub(\"<br/>\",\"\",text)\n",
    "    return new_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace key digits with actual usernames \n",
    "def replace_username(dict_username, dict_review):\n",
    "\n",
    "    new_dict = dict((dict_username[key], value) for (key, value) in dict_review.items())\n",
    "    return new_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove null entries in dictionary \n",
    "def remove_nulls_from_dict(dict_):\n",
    "\n",
    "    return {user: value for user,value in dict_.items() if value!='Null'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep popular reviewers \n",
    "def keep_popular_reviewers(dict_t, popular_list):\n",
    "\n",
    "    new_dict = {x:v for x,v in dict_t.items() if x in popular_list } # if v in popular_list}\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce dataset \n",
    "def reduce_dict(dict_):\n",
    "    if len(dict_)>=50:\n",
    "        return dict(random.sample(dict_.items(), 50))\n",
    "    else:\n",
    "        return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "#using a regex tokenizer because I am cool like that \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def lemmatize_text(some_dict):\n",
    "\n",
    "    dialogue_lemmatized = {ch:lemmatizer.lemmatize(w.lower()) for ch,w in some_dict.items()}\n",
    "    \n",
    "    return dialogue_lemmatized \n",
    "\n",
    "def clean_text(dialogue_lemmatized):\n",
    "    \n",
    "    #remove funny characters \n",
    "    dialogue_clean = {ch:re.sub(r'[^A-Za-z0-9]+', ' ',text)  for ch,text in dialogue_lemmatized.items() }\n",
    "    \n",
    "    return dialogue_clean \n",
    "\n",
    "#tokenize reviews into word tokens \n",
    "def tokenize_dialogue(dialogue_clean):\n",
    "    \n",
    "    dialogue_tokenized = {ch: word_tokenize(text) for ch, text in dialogue_clean.items()}\n",
    "\n",
    "    return dialogue_tokenized\n",
    "\n",
    "#keep only the ones who are in the labmt dictionary \n",
    "def labmt_tokens(dialogue_tokenized):\n",
    "\n",
    "    dialogue_tokenized_v2 = {k:v for k,v in dialogue_tokenized.items() if v}\n",
    "    labmt_tokens = {ch :[float(labMT[token][1]) for token in text if token in list(labMT.keys())] for ch, text in dialogue_tokenized_v2.items()}\n",
    "    \n",
    "    return labmt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain average labmt sentiment for each user\n",
    "def compute_sentiment_per_user(user_dict):\n",
    "\n",
    "    return {user:sum(list_values)/len(list_values) if len(list_values)>0 else 0 for user,list_values in user_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_sent(dict_tokens):\n",
    "\n",
    "    avg_sent_per_user = {ch:sum(token_sent)/len(token_sent) for ch,token_sent in dict_tokens.items()}\n",
    "    \n",
    "    return sum(list(avg_sent_per_user.values()))/len(avg_sent_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_entries(dict):\n",
    "\n",
    "    return  {ch: value  for ch,value in dict.items() if len(value)>0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "The steps below are meant to process the different columns of the dataframe into a useable form so we can proceed and compute the sentiment score for each reviewer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['review_final'] = movie_reviews.apply(lambda x: process_text(x.review),axis=1)\n",
    "movie_reviews['review_final'] = movie_reviews.apply(lambda x: replace_username(x.username,x.review_final), axis=1)\n",
    "movie_reviews['rating_final'] = movie_reviews.apply(lambda x: replace_username(x.username,x.rating), axis=1)\n",
    "movie_reviews['title_final'] = movie_reviews.apply(lambda x: replace_username(x.username,x.title), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = movie_reviews[['original_title','rating_final','title_final','review_final']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['rating_final'] = movie_reviews.apply(lambda x: remove_nulls_from_dict(x.rating_final), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['review_final'] = movie_reviews.apply(lambda x: remove_nulls_from_dict(x.review_final), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tmdb_5000 , reviews_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['rating_final'] = movie_reviews.apply(lambda x: keep_popular_reviewers(x.rating_final,popular_reviewers_list), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['no_ratings'] = movie_reviews.apply(lambda x: len(x.rating_final), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihae\\AppData\\Local\\Temp\\ipykernel_23128\\1490526405.py:3: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  return dict(random.sample(dict_.items(), 50))\n"
     ]
    }
   ],
   "source": [
    "movie_reviews['rating_final'] = movie_reviews.apply(lambda x: reduce_dict(x.rating_final), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews[['original_title','rating_final']].to_pickle('ratings_only.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['review_final'] = movie_reviews.apply(lambda x: keep_popular_reviewers(x.review_final,list(x.rating_final.keys())), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihae\\AppData\\Local\\Temp\\ipykernel_23128\\1490526405.py:3: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  return dict(random.sample(dict_.items(), 50))\n"
     ]
    }
   ],
   "source": [
    "movie_reviews['review_final'] = movie_reviews.apply(lambda x: reduce_dict(x.review_final), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis \n",
    "\n",
    "_The labMT dictionary (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752) contains a collection of 10.222 words which were evaluated by users on Mechanical Turk. The words are the keys in the labMT dictionary. \n",
    "The keys are ordered in the dictionary in an descending order according to the average happiness ranking.\n",
    "Each word (key) has associated a list with 7 elements where the first element in the list represents the ranking followed by the average happiness ranking (as evaluated by 50 users), the standard deviation of happiness, twitter rank, google books rank, new york times rank and finally the music lyrics rank_. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['reviews_lemmatized'] = movie_reviews.apply(lambda x: lemmatize_text(x.review_final),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['reviews_lemmatized_clean'] =  movie_reviews.apply(lambda x: clean_text(x.reviews_lemmatized),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['tokenized_reviews'] = movie_reviews.apply(lambda x: tokenize_dialogue(x.reviews_lemmatized_clean),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['labmt_tokens'] = movie_reviews.apply(lambda x: labmt_tokens(x.tokenized_reviews),axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews['user_sentiment'] = movie_reviews.apply(lambda x: compute_sentiment_per_user(x.labmt_tokens),axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews[['original_title','rating_final','user_sentiment']].to_pickle('rating_and_sentiment.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.to_pickle('labmt_sentiment.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a5f12a4d41add377cb6616dca9389fa9368a6ad393a55ea33793a3908b73d5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
